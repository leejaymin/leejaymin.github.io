---
---

string{aps = {American Physical Society,}}




@article{jinse2026,
    abbr={Access},
    title={CNN Compression via Channel-Wise Variance-Based Filter Pruning},
    author={Jinse Kwon and Jemin Lee and Sihyeong Park and Hyungshin Kim},
    journal={IEEE Access (JCR24 IF: 3.6, Top 35.05\%, Q2), 2026 accepted, ISSN: 2169-3536},
    month = {4},
    year={2026},
    pdf = {https://ieeexplore.ieee.org/document/11352833},
    abstract= {최근 몇 년간 경량 합성곱 신경망(lightweight convolutional neural networks)에 대한 관심이 증가함에 따라, 정확도를 유지하면서 모델 크기와 추론 비용을 줄이기 위한 가지치기(pruning) 기법에 대한 연구가 활발히 진행되어 왔다. 기존의 구조적 가지치기 기법들은 L1/L2 노름이나 기하학적 중앙값과 같은 크기 기반(magnitude-based) 지표에 의존하는 경우가 많으며, 이러한 방법들은 종종 유사한 필터 순위를 생성한다. 이로 인해 도출된 중요도 점수가 얼마나 고유하고 정보성이 있는지에 대한 우려가 제기되어 왔다.
본 연구에서는 합성곱 계층 내 가중치의 분산 분포를 통해 필터의 중요도를 평가하는 분산 기반 가지치기 기법을 제안한다. 제안하는 방법은 기존의 크기 기반 접근법과 달리, 노름 기반 방법에서 간과되는 보완적인 정보를 포착한다. 이론적 분석을 통해, 가중 분산(weighted variance)이 기존 노름 기반 기준과 구별되는 특성을 지님을 보인다.
제안 기법의 효과를 검증하기 위해 ImageNet 데이터셋을 사용한 포괄적인 실험을 수행하였다. L1-노름 가지치기, L2-노름, 기하학적 중앙값, HRank, RASP, AutoSlim, 그리고 Taylor 근사 기반 가지치기 기법을 포함한 다양한 기준 방법들과 비교하였다. 그 결과, 제안하는 방법은 ResNet-50 기준 모델 대비 연산량의 절반 이하인 2G MACs만으로도 최대 76.2%의 Top-1 정확도를 달성하였다.}
}

@article{kim2025fgcs,
  abbr={FGCS},
  title={Towards an Efficient Dataflow-flexible Accelerator by Finding Optimal Dataflows of DNNs},
  author={Hyunjun Kim and Whoi Ree Ha and Yongseok Lee and Dongju Lee and Jongwon Lee and Deumji Woo and Jemin Lee and Yongin Kwon and Yunheung Paek},
  journal={Future Generation Computer Systems, 1 Mar 2026, 108123, JCR24 IF 6.1 Top 9.86%, ISSN: 0167-739X},
  month = {3},
  year={2026},
  pdf = {https://doi.org/10.1016/j.future.2025.108123},
  abstract= {이 논문은 여러 딥 뉴럴 네트워크(DNN) 모델의 연산을 처리하는 데 기존 이종 데이터플로우 가속기(Heterogeneous Dataflow Accelerator, HDA)가 갖는 한계를 해결하기 위한 새로운 데이터플로우 유연형 가속기 설계를 제안한다. 제안된 설계는 기존 연구와 비교하여 더 높은 데이터플로우 유연성과 향상된 효율성을 제공한다.
가속기는 대표적인 데이터플로우를 일정 수만큼 **운영 모드(operating mode)**로 고정적으로 구현하며, 이들 간을 동적으로 전환해가며 연산을 수행한다. 또한 설계 탐색(DSE, Design Space Exploration) 도구를 활용해 후보 데이터플로우들의 효율성을 평가하고, 최적의 운영 모드 수와 구성 유형을 결정한다.
대상 DNN 모델의 각 레이어는 다양한 운영 모드를 적용해 평가되며, 레이어별로 최적의 운영 모드를 선택하게 된다. 추가적으로, 다수의 데이터플로우를 지원하면서 발생하는 오버헤드를 줄이기 위해 두 가지 최적화 기법이 도입된다.
첫 번째 기법은 데이터플로우 전환 횟수를 최소화해 전환 시 발생하는 큰 오버헤드를 줄이는 것이다.
두 번째 기법은 여러 데이터플로우를 지원하기 위해 필요한 하드웨어 구성 요소들 중 중복되는 부분을 식별하고 재사용을 극대화하는 것이다. 이를 통해 데이터플로우 유연형 가속기 설계에서 중요한 문제인 칩 면적 증가를 최소화한다.
실험 결과, 제안된 알고리즘은 높은 효율을 유지하면서 더 큰 데이터플로우 유연성을 제공함을 보였다. HDA와 비교했을 때, 본 설계는 평균 34.6%의 지연(latency) 감소를 달성하면서도 칩 면적은 6.4% 증가, 에너지 오버헤드는 무시할 수준에 그쳤다.}
}
% 25.9.6

@article{cha2025targetaware,
  abbr={IEEE TMC},
  title={Target-Aware Neural Network Execution via Compiler-Guided Pruning},
  author={JooHyoung Cha and Taeho Kim and Jemin Lee* and Sangtae Ha and Yongin Kwon*},
  journal={IEEE Transactions on Mobile Computing (to appear), Accepted, JCR24 IF 9.2 Top 3.29% Nov. 9, 2025, ISSN: 1536-1233},
  day = {9},
  month = {11},
  year={2025},
  selected={true},
  pdf = {https://ieeexplore.ieee.org/document/11240555},
  abstract = {모바일 기기는 이미지 분류나 음성 인식과 같은 다양한 목적을 위해 딥러닝 모델을 실행한다. 그러나 모바일 기기의 자원 제약으로 인해, 연구자들은 모델 프루닝(pruning)을 통해 경량화된 딥 뉴럴 네트워크(DNN) 모델을 만들거나, 컴파일러 최적화를 통해 효율적인 코드를 생성하는 데 주력해 왔다.
하지만 모델 압축과 컴파일러 자동 튜닝(auto-tuning)을 단순히 결합하는 방식은 특정 타깃 장치에서 가장 효율적인 모델을 만들지 못하는 경우가 많음이 관찰되었다.
이 문제를 해결하기 위해 우리는 CPrune을 제안한다. CPrune은 요구되는 정확도를 충족해야 하는 애플리케이션을 지원하기 위해, 타깃 장치 특화 효율적 실행을 위한 컴파일러 기반 정보 활용 모델 프루닝(compiler-informed model pruning) 기법이다.
또한 실제 배포 환경에서의 자원 또는 지연(latency) 제약을 고려하여, 우리는 RB-CPrune을 도입한다. RB-CPrune은 학습된 **지연 예측기(latency estimator)**를 활용해 반복적인 튜닝 과정을 제거한 예측 기반(prdictive) 변형 기법이다.
CPrune은 컴파일러 튜닝 과정에서 구축된 서브그래프의 구조적 정보를 기반으로 프루닝을 수행함으로써 경량 DNN 모델을 생성한다.
실험 결과, CPrune은 정확도 요구 조건을 충족하면서도 최첨단 TVM auto-tune 대비 최대 2.73배 빠른 DNN 실행 속도를 달성함을 보여주었다.}
}

@inproceedings{kim2026iptqvit,
  abbr={WACV},
  title={IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers},
  author={Gihwan Kim and Jemin Lee and Hyungshin Kim},
  booktitle={IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026, Round 1 acceptance rate <span class="label label-warning">6.4%</span>, Round 2 acceptance rate <span class="label label-warning">41.1%</span>, Overall acceptance rate <span class="label label-warning">26.4%</span>},
  month = {2},
  year={2026},
  selected={true},
  pdf = {https://arxiv.org/abs/2511.15369},
  abstract = {}
}

@inproceedings{caseswip2025,
    abbr={CASES},
    title={I-FlashAttention: Fully Integer Fused Attention for Efficient Vision Transformers (WIP)},
    author={Sehyeon Oh and Yongin Kwon and Jemin Lee*},
    booktitle={ACM International Conference on Compilers, Architectures, and Synthesis for Embedded Systems (ESWEEK CASES 2025), Acceptance Rate 57.1% (4 papers accepted out of 7 invited) September 30, 2025.},
    month = {9},
    year={2025},
    organization={IEEE},
    pdf = {https://doi.org/10.1145/3742872.3757072}
}


@inproceedings{cases25,
  abbr={CASES},
  award = {Artifacts},
  title={Luthier: Bridging Auto-Tuning and Vendor Libraries for Efficient Deep Learning Inference},
  author={Yongin Kwon and JooHyoung Cha and Sehyeon Oh and Misun Yu and Jeman Park and Jemin Lee*},
  booktitle = {ACM International Conference on Compilers, Architectures, and Synthesis for Embedded Systems (ESWEEK CASES 2025)
  (NRF BK21+ IF: 2, Acceptance Rate 28.2% (20 papers accepted out of 71 submitted)) and
  ACM Transactions on Embedded Computing Systems (TECS) Vol. 24, No. 5s, pp. 1--23 ISSN:1539-9087, September 29, 2025.},
  pages={1--20},
  month = {9},
  year={2025},
  selected={true},
  pdf = {https://dl.acm.org/doi/10.1145/3759916},
  poster = {luthier_poster.pdf},
  slides = {luthier_slides.pdf},
  code = {https://gitlab.com/ones-ai/Luthier},
  doi = {10.1145/3759916},
  abstract = {최근 딥러닝 컴파일러는 텐서 프로그래밍에서 최적의 커널 구성을 처음부터 탐색하는 자동 튜닝 방식을 주로 채택하는데, 이는 작업당 수십 시간이 소요되며 비대칭 멀티코어 프로세서에서의 병렬 컴퓨팅에 필수적인 최적화 요소를 간과합니다. 한편 하드웨어 벤더의 수동 최적화 추론 라이브러리는 높은 성능을 제공하지만 신흥 모델에 필요한 유연성과 자동화가 부족합니다. 이러한 격차를 해소하기 위해 우리는 기존 추론 라이브러리에서 최적의 커널을 선별하여 탐색 공간을 크게 축소하고, 비용 모델 기반 프로파일링을 활용해 병렬 컴퓨팅에 가장 효율적인 작업 부하 분배를 신속히 결정하는 Luthier를 제안합니다. 그 결과 Luthier는 ArmNN, AutoTVM, Ansor, ONNXRuntime, TFLite 대비 평균 튜닝 시간을 95% 단축하면서, CPU와 GPU 모두에서 컨볼루션 기반 비전 모델과 트랜스포머 기반 언어 모델(BERT, GPT)에서 최대 2.0배 빠른 실행 속도를 달성합니다.
}
}

@inproceedings{acvr25_1,
  abbr={ICCV W.},
  title={Design Practices and Lessons from Deploying On-device Vision-Language Interaction in Robotic Guide Dogs},
  author={Jinse Kwon and Jemin Lee* and Yongin Kwon*},
  booktitle = {ICCV Workshop ACVR 2025},
  pages={1--10},
  month = {7},
  year={2025},
  pdf = {https://openaccess.thecvf.com/content/ICCV2025W/ACVR/papers/Kwon_Design_Practices_and_Lessons_from_Deploying_On-device_Vision-Language_Interaction_in_ICCVW_2025_paper.pdf},
  code = {}
}

@inproceedings{acvr25_2,
  abbr={ICCV W.},
  title={TriPlanNet: Triangle Path Planning Network for A Variable Truss Robot with Deep Learning},
  author={Choonghan Lee and Leah Harris and Sehyeon Oh and Juhyung Cha and Jemin Lee and Yongin Kwon and Andrew Jang-ho Bae},
  booktitle = {ICCV Workshop ACVR 2025},
  pages={1--10},
  month = {7},
  year={2025},
  pdf = {https://openaccess.thecvf.com/content/ICCV2025W/ACVR/papers/Lee_TriPlanNet_Triangle_Path_Planning_Network_for_A_Variable_Truss_Robot_ICCVW_2025_paper.pdf},
  code = {}
}

@inproceedings{lee25survey,
  abbr={Preprint},
  title={A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency},
  author = {Sihyeong Park and Sungryeol Jeon and Chaelyn Lee and Seokhun Jeon and Byung-Soo Kim and Jemin Lee*},
  booktitle = {Preprint on ArXiv 2505.01658 May 3, 2025},
  pages={1--65},
  month = {5},
  year={2025},
  pdf = {https://arxiv.org/abs/2505.01658},
  code = {https://github.com/sihyeong/Awesome-LLM-Inference-Engine}
}

@inproceedings{lee25ijcai,
  abbr={IJCAI},
  title={Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant},
  author={Jemin Lee and Sihyeong Park and Jinse Kwon and Jihun Oh and Yongin Kwon*},
  booktitle={International Joint Conferences on Artificial Intelligence (IJCAI) Aug. 22, 2025, NRF BK21+ IF: 4, Acceptance Rate <span class="label label-warning">19.3%</span> (1042 papers accepted out of 5404 submitted).},
  pages={8113--8121},
  month = {8},
  year={2025},
  selected={true},
  pdf = {https://doi.org/10.24963/ijcai.2025/902},
  code = {https://gitlab.com/ones-ai/eval-quant-llms},
  abstract = {양자화는 대규모 및 소규모 언어 모델의 비용 효율적 배포를 위한 유망한 해결책으로 주목받고 있습니다. 그러나 기존 연구 대부분은 퍼플렉시티나 기초 지식 작업에 국한되어 있으며 Llama-3.3과 같은 최신 모델에 대한 포괄적 평가가 부족했습니다. 본 논문에서는 10억에서 4050억 매개변수에 이르는 명령어 학습 모델을 대상으로 13개 데이터셋에 걸쳐 4가지 양자화 기법을 적용하여 포괄적 평가를 수행합니다.
우리의 연구 결과는 다음과 같다: (1) 양자화 모델은 일반적으로 더 작은 FP16 기준 모델을 능가하지만, 명령어 수행 및 환각 탐지에서는 종종 어려움을 겪는다; (2) FP8은 모든 작업에서 가장 견고한 옵션으로 꾸준히 나타났으며, 가중치 전용 양자화에서는 AWQ가 GPTQ보다 우수한 성능을 보이는 경향이 있다; (3) 소규모 모델은 4비트 양자화 시 심각한 정확도 하락을 겪을 수 있는 반면, 70B 규모 모델은 안정적인 성능을 유지한다; (4) 특히, 어려운 작업이 항상 가장 큰 정확도 손실을 보이는 것은 아니며, 이는 양자화가 모델의 본질적 약점을 증폭시키기보다는 가려진 부분을 드러낸다는 점을 시사한다.
4비트 양자화 시 심각한 정확도 하락을 보일 수 있으나, 70B 규모 모델은 안정적인 성능을 유지함; (4) 특히 어려운 과제가 항상 가장 큰 정확도 손실을 보이는 것은 아니며, 이는 양자화가 단순히 과제 난이도와 상관관계를 가지는 것이 아니라 모델의 내재적 약점을 증폭시킨다는 점을 시사함; (5) LLM 기반 평가 도구(MTBench)는 코딩 및 STEM 과제에서 상당한 성능 저하를 강조하지만, 추론에서는 가끔 개선을 보고함.}
}
%  16-12 Aug. 2025

@inproceedings{lectes25,
  abbr={LCTES},
  title={Multi-Level Machine Learning-Guided Autotuning for Efficient Code Generation on a Deep Learning Accelerator},
  author={JooHyoung Cha and Munyoung Lee and Jinse Kwon and Jemin Lee and Yongin Kwon},
  booktitle={The 26th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES)
  Jun. 17, 2025, To appear, NRF BK21+ IF: 2, Acceptance Rate <span class="label label-warning">38%</span> (16 papers accepted out of 42 submitted).},
  pages={134--145},
  month = {6},
  year={2025},
  selected={true},
  pdf = {https://doi.org/10.1145/3735452.3735538},
  abstract = {딥 러닝 모델의 복잡성 증가는 특히 딥 러닝 가속기를 위한 특수 하드웨어 및 소프트웨어 최적화를 필요로 합니다. 기계 학습 기반 자동 튜닝 방법이 수동 작업을 줄이는 유망한 해결책으로 부상했지만, 템플릿 기반 및 템플릿 프리 접근법 모두 유효하지 않은 구성 프로파일링으로 인해 튜닝 시간이 길어지는 문제점을 안고 있으며, 이는 런타임 오류로 이어질 수 있습니다. 이 문제를 해결하기 위해 효율성과 견고성을 개선하도록 설계된 다단계 머신 러닝 기반 자동 튜닝 기술인 ML2Tuner를 제안합니다. ML2Tuner는 두 가지 핵심 아이디어를 도입합니다: (1) 프로파일링 전에 무효한 구성을 걸러내는 유효성 예측 모델, (2) 컴파일 과정에서 추출된 숨겨진 특징을 활용하는 고급 성능 예측 모델입니다. 확장된 VTA 가속기에서의 실험 결과, ML2Tuner는 TVM 유사 접근법이 요구하는 샘플의 12.3%만을 사용하여 동등한 성능 향상을 달성하며, 무효 프로파일링 시도를 평균 60.8% 감소시킵니다. 이는 무효 구성을 걸러내어 자동 튜닝 성능을 향상시킬 수 있는 잠재력을 보여줍니다.}
}
%  16-17 Jun. 2025

@inproceedings{QuantuneV2,
  abbr={FGCS},
  title={QuantuneV2: Compiler-based local metric-driven mixed precision quantization for practical embedded AI applications},
  author={Jeongseok Kim** and Jemin Lee** and Yongin Kwon and Daeyoung Kim},
  booktitle={Future Generation Computer Systems Volume 166, May 1, 2025, 107718 (JCR24 IF: 6.1, Top 9.86%, Q1), ISSN: 0167-739X},
  pages={1--18},
  day = {1},
  month = {5},
  year={2025},
  pdf = {https://doi.org/10.1016/j.future.2025.107718},
  abstract = {혼합 정밀도 양자화 기법은 정확도 저하를 최소화하면서 모델 크기를 줄이기 위해 제안되어 왔다. 그러나 기존 연구들은 재학습이 필요하며, 컴파일 과정에서 발생하는 계산 오버헤드와 중간 표현(IR)을 고려하지 않아 컴파일러 수준에서의 적용에 한계가 있다. 여기서 말하는 계산 오버헤드는 추론 과정에서 빈번하게 수행되는 양자화 및 비양자화 연산으로 인해 발생하는 런타임 지연을 의미한다. 이러한 연산을 개별 연산자 단위로 수행할 경우 런타임이 크게 증가한다.
  이 문제를 해결하기 위해, 우리는 실질적인 임베디드 AI 응용을 위해 설계된 컴파일러 기반 혼합 정밀도 양자화 방법인 QuantuneV2를 제안한다. QuantuneV2는 양자화 전과 양자화 후 단 두 번만 추론을 수행하며, 모델 파라미터 수에 비례해 선형적으로 증가하는 계산 복잡도를 가진다. 또한 가중치, 활성값, 신호대양자화잡음비(SQNR), 평균제곱오차(MSE)와 같은 로컬 메트릭을 활용하여 민감도 분석을 더욱 안정적으로 만들었다. 아울러 최적의 IR을 선택하고 연산자 융합(operator fusion)을 적용하여 계산 오버헤드를 줄였다.
  실험 결과, QuantuneV2는 ResNet18v1, ResNet50v1, SqueezeNetv1, VGGNet, MobileNetv2 등 다섯 가지 모델에서 기존 방법 대비 최대 10.28%의 정확도 향상과 12.52%의 속도 향상을 달성했다. 이는 QuantuneV2가 계산 효율성을 유지하면서 모델 성능을 향상시켜 임베디드 AI 환경에 적합함을 보여준다.}
}

@inproceedings{sensors25,
  abbr={Sensors},
  award = {Invited},
  title={Optimizing Real-Time Object Detection in a Multi NPU Systems},
  author={Sehyeon Oh and Yongin Kwon  and Jemin Lee*},
  booktitle={MDPI Sensors, Volume 25, Issue 5, pp. 1376 March 1 2025 EISSN 1424-8220 (JCR23 IF: 3.4, JCR23 Top 30.9%, Q2)},
  pages={1--13},
  day = {1},
  month = {3},
  year={2025},
  pdf = {https://doi.org/10.3390/s25051376},
  abstract = {실시간 객체 탐지는 높은 처리량과 낮은 지연 시간을 요구하므로, 하드웨어 가속기의 사용이 필수적이다. NPU는 딥러닝 모델 계산을 가속하기 위해 설계된 특수 하드웨어로, 기존 CPU나 GPU보다 더 높은 에너지 효율과 병렬 처리 성능을 제공한다. 특히 실시간 처리가 필요한 애플리케이션에서 지연 시간을 줄이고 처리 속도를 향상시키는 데 중요한 역할을 한다.
본 논문에서는 YOLOv3 기반의 실시간 객체 탐지 시스템을 구축하고, Neubla의 Antara NPU를 활용하여 성능 최적화를 위한 두 가지 접근법을 제안한다. 첫째, 더블 버퍼링(double buffering)을 통해 CPU가 데이터를 미리 처리하도록 하여 NPU 추론의 연속성을 확보한다. 둘째, 다중 NPU 환경에서 큐(queue) 기반 처리 방식으로 작업을 NPU 간에 분배하며, 암달의 법칙(Amdahl’s law)을 이용해 성능 한계를 분석한다.
실험 결과, CPU만 사용하는 환경과 비교했을 때, 단일 버퍼링(single buffering)에서 NPU를 적용하면 처리량이 2.13배 향상되었고, 더블 버퍼링에서는 3.35배, 다중 NPU 환경에서는 4.81배 향상되었다. 지연 시간(latency)은 단일 및 더블 버퍼링 환경에서 1.6배, 다중 NPU 환경에서 1.18배 감소하였다. 정확도는 CPU에서 31.4 mAP, NPU에서 31.8 mAP로 거의 동일하게 유지되었다.}
}


@inproceedings{AIcompS24oh,
  abbr={AIcompS},
  award = {Distinguished},
  title={Optimizing Real-Time Object Detection in a Multi NPU System with Double Buffering and Queue-Based Processing},
  author={Sehyeon Oh and Yongin Kwon  and Jemin Lee*},
  booktitle={The 1st International Conference on Artificial Intelligence Computing and Systems},
  pages={1--4},
  day = {16},
  month = {12},
  year={2024},
  pdf = {https://leejaymin.github.io/assets/pdf/aicomps_oh.pdf},
}

@inproceedings{AIcompS24park,
  abbr={AIcompS},
  title={A Review on Proprietary Accelerators for Large Language Models},
  author={Sihyeong Park and Jemin Lee and Byung-Soo Kim and Seokhun Jeon},
  booktitle={The 1st International Conference on Artificial Intelligence Computing and Systems},
  pages={1--4},
  day = {16},
  month = {12},
  year={2024},
  pdf = {https://aicomps.kips.or.kr/},
}

@inproceedings{ml2tuner,
  abbr={NeurIPS W.},
  title={ML^2Tuner: Efficient Code Tuning via Multi-Level Machine Learning Models},
  author={JooHyoung Cha and Munyoung Lee and Jinse Kwon and Jubin Lee and Jemin Lee and Yongin Kwon},
  booktitle={Machine Learning for Systems Workshop at NeurIPS},
  pages={1--12},
  month = {12},
  year={2024},
  pdf = {https://mlforsystems.org/assets/papers/neurips2024/paper6.pdf},
}

@article{jemin2024Qhyvit,
    abbr={IEEE IoT J.},
    title={Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction for IoT Systems},
    author={Jemin Lee and Yongin Kwon and Misun Yu and Jeman Park and Hwanjun Song},
    journal={IEEE Internet of Things Journal Vol. 11, Issue 22, pp.36384-36396, ISSN: 2327-4662, Nov. 15, 2024 (JCR24 IF: 8.9 Top 4.07%)},
    publisher={IEEE},
    pages={36384--36396},
    day = {15},
    month = {11},
    year={2024},
    selected={true},
    pdf={https://doi.org/10.1109/JIOT.2024.3403844},
    code = {https://gitlab.com/ones-ai/q-hyvit},
    abstract = {최근 비전 트랜스포머(ViT)는 분류, 탐지, 세분화 등 다양한 응용 분야에서 합성곱 신경망(CNN)을 대체하며 뛰어난 성능을 보이고 있다. 그러나 ViT의 높은 계산 요구량은 광범위한 활용을 어렵게 만든다. 이러한 문제를 해결하기 위해 연구자들은 합성곱 레이어와 트랜스포머 레이어를 결합하고, 선형 복잡도의 최적화된 어텐션 계산을 사용하는 효율적인 하이브리드 트랜스포머 아키텍처를 제안해 왔다. 또한 사후학습 양자화(PTQ)가 계산 부담을 줄이는 방법으로 제시되었다. 모바일 장치에서 ViT의 최적 가속을 달성하려면 양자화 기법과 효율적인 하이브리드 트랜스포머 구조를 전략적으로 통합해야 한다. 그러나 지금까지 효율적인 하이브리드 트랜스포머에 양자화를 적용한 연구는 없었다.
    이 논문에서는 기존의 ViT용 PTQ 기법을 효율적인 하이브리드 트랜스포머에 적용할 경우 정확도가 크게 떨어진다는 사실을 발견했으며, 이는 매우 동적인 값 범위, 제로포인트 오버플로, 다양한 정규화 방식, 5M 미만의 제한된 모델 파라미터 수 등 네 가지 문제 때문이라고 밝힌다. 이러한 문제를 해결하기 위해 우리는 새로운 PTQ 기법을 제안하며, 이는 MobileViTv1, MobileViTv2, Mobile-Former, EfficientFormerV1, EfficientFormerV2 등 효율적인 하이브리드 ViT를 최초로 양자화한 방법이다.
    우리 방법은 기존 PTQ 기법(EasyQuant, FQ-ViT, PTQ4ViT, RepQ-ViT) 대비 8비트에서 평균 17.73%, 6비트에서 평균 29.75%의 성능 향상을 달성했다. 코드는 https://gitlab.com/ones-ai/q-hyvit 에서 공개할 예정이다.}
}

@inproceedings{park2024etri,
  abbr={ETRI J.},
  title={NEST-C: A Deep Learning Compiler Framework for Heterogeneous Computing Systems with AI Accelerators},
  author={Jeman Park and Misun Yu and Jinse Kwon and Junmo Park and Jemin Lee* and Yongin Kwon*},
  booktitle={ETRI Journal Vol. 46 Issue 5, pp.851-864 ISSN: 1225-6463 (JCR24 IF 1.6, 70%) Oct. 28, 2024},
  pages={851--864},
  day = {28},
  month = {10},
  year={2024},
  pdf = {https://doi.org/10.4218/etrij.2024-0139},
  code = {https://gitlab.com/ones-ai/nest-compiler},
  abstract = {딥 러닝(DL)은 인공 지능(AI)을 크게 발전시켰으나, PyTorch, ONNX, TensorFlow와 같은 프레임워크는 범용 GPU에 최적화되어 있어 신경 처리 장치(NPU) 및 메모리 내 처리(PIM) 장치와 같은 특수 가속기에서 비효율적입니다. 이러한 가속기는 처리량과 에너지 효율을 모두 최적화하도록 설계되었지만, 보다 맞춤화된 최적화가 필요합니다. 이러한 한계를 해결하기 위해 우리는 다양한 AI 가속기에서 모델 배포 및 성능을 개선하는 새로운 DL 프레임워크인 NEST 컴파일러(NEST-C)를 제안합니다. NEST-C는 프로파일링 기반 양자화, 동적 그래프 분할, 다단계 중간 표현(IR) 통합을 활용하여 다양한 하드웨어 플랫폼에서 효율적인 실행을 가능하게 합니다. 연구 결과 NEST-C는 다양한 AI 가속기에서 계산 효율성과 적응성을 크게 향상시켜 더 높은 처리량, 낮은 지연 시간, 개선된 자원 활용도, 향상된 모델 이식성을 달성합니다. 이러한 이점은 현대 AI 애플리케이션에서 더 효율적인 DL 모델 배포에 기여합니다.}
}

@inproceedings{gihwan2024Eccv,
  abbr={ECCV W.},
  title={Mixed Non-linear Quantization for Vision Transformers},
  author={Gihwan Kim** and Jemin Lee** and Sihyeong Park and Yongin Kwon and Hyungshin Kim},
  booktitle={ECCV Workshop CADL},
  pages={1--17},
  month = {9},
  year={2024},
  pdf = {https://arxiv.org/abs/2407.18437},
  code = {https://gitlab.com/ones-ai/mixed-non-linear-quantization}
}

@inproceedings{VisualIROS2024,
  abbr={IROS},
  title={Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation},
  author={Joonhyung Lee and Sangbeom Park and Yongin Kwon and Jemin Lee and Minwook Ahn and Sungjoon Choi},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024), 14 Oct. 2024},
  pages={1--8},
  day = {14},
  month = {10},
  year={2024},
  selected={true},
  pdf = {https://arxiv.org/abs/2403.11513},
  code = {https://github.com/joonhyung-lee/vpi},
  website = {https://joonhyung-lee.github.io/vpi/}
}


@article{kwon2023pipelining,
  abbr={IEEE ESL},
  title={Pipelining of a Mobile SoC and an External NPU for Accelerating CNN Inference},
  author={Kwon, Jinse and Lee, Jemin and Kim, Hyungshin},
  journal={IEEE Embedded Systems Letters, Vol. 16 Issue 2 pp. 150-153, June 2024 ISSN 1943-0663 (JCR24 IF 2, 55.08), doi: https://doi.org/10.1109/LES.2023.3305016},
  month = {6},
  year={2024},
  publisher={IEEE},
  pdf={https://doi.org/10.1109/LES.2023.3305016},
    abstract = {하드웨어와 소프트웨어의 공동 발전에 따라 컨볼루션 신경망(CNN) 알고리즘이 에지 디바이스에 점점 더 많이 배포되고 있습니다. 리소스가 제한된 디바이스에 CNN을 배포하려면 종종 CPU와 GPU의 최적화가 필요합니다. 신경망 처리 장치(NPU)와 같은 전용 하드웨어가 성공적으로 도입되었지만, CPU, GPU 및 NPU 간의 협력적 방법은 아직 미성숙한 상태입니다. 본 논문에서는 모바일 시스템온칩(SoC)과 외부 NPU(eNPU)의 통합을 최적화하여 조화로운 파이프라이닝을 달성하고 추론 속도 및 처리량을 향상시키기 위한 두 가지 접근법을 제안한다. 첫 번째 접근법은 호스트 측에서 레이어별 최적 라이브러리를 할당하기 위한 기본 선형 대수 서브프로그램 라이브러리 검색 방식을 포함하며, 두 번째 접근법은 모델 슬라이스 포인트를 탐색하여 성능을 최적화한다. 자동 할당되는 계산 라이브러리로 CPU 기반 NNPACK, OpenBLAS 및 GPU 기반 CLBlast를 활용합니다. 전체 신경망(NN)은 NN 레이어 특성과 하드웨어 성능을 기반으로 두 부분으로 최적 분할됩니다. Hikey-970, Hikey-960, Firefly-rk3399 등 다양한 모바일 기기에서 알고리즘을 평가했습니다. 실험을 통해 제안된 파이프라인 추론 방식이 eNPU 및 SoC에서의 병렬 실행 대비 지연 시간을 10% 감소시키고 처리량을 17% 이상 증가시킨다는 점을 입증했습니다.}
}

@inproceedings{VisualICRAW,
  abbr={ICRA W.},
  title={Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation},
  author={Joonhyung Lee and Sangbeom Park and Yongin Kwon and Jemin Lee and Minwook Ahn and Sungjoon Choi},
  booktitle={First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024},
  pages={1--6},
  year={2024},
  pdf = {https://openreview.net/forum?id=D2e4URvhpP},
}

@inproceedings{acltuner,
  abbr={NeurIPS W.},
  title={ACLTuner: A Profiling-Driven Fast Tuning to Optimize Deep Learning Inference},
  author={Yongin Kwon and Joo Hyoung Cha and Jubin Lee and Misun Yu and Jeman Park and Jemin Lee*},
  booktitle={Machine Learning for Systems Workshop at NeurIPS},
  pages={1--4},
  year={2023},
  pdf = {https://openreview.net/pdf?id=k0FIPHpeR4},
}

@article{yu2023partitiontuner,
    abbr={ETRI J.},
    title={PartitionTuner: An operator scheduler for deep-learning compilers supporting multiple heterogeneous processing units},
    author={Yu, Misun and Kwon, Yongin and Lee, Jemin and Park, Jeman and Park, Junmo and Kim, Taeho},
    journal={ETRI Journal Vol. 45 No. 2, Apr. 20, 2023 (JCR23 IF: 1.3) ISSN: 1225-6463, doi: https://doi.org/10.4218/etrij.2021-0446},
    volume={45},
    number={2},
    pages={318--328},
    day = {20},
    month = {4},
    year={2023},
    publisher={Wiley Online Library},
    selected={false},
    pdf={https://doi.org/10.4218/etrij.2021-0446},
    abstract = {최근 모바일 플랫폼과 같은 임베디드 시스템은 중앙 처리 장치(CPU)와 신경망 처리 장치(NPU) 등 병렬로 작동할 수 있는 다중 처리 장치를 갖추고 있습니다. 딥 러닝 컴파일러를 활용하면 딥 신경망(DNN)으로부터 이러한 임베디드 시스템에 최적화된 기계 코드를 생성할 수 있습니다. 그러나 지금까지 제안된 딥러닝 컴파일러는 단일 처리 장치에서 DNN 연산자를 순차적으로 실행하는 코드나 그래픽 처리 장치(GPU)용 병렬 코드만을 생성합니다. 본 연구에서는 CPU와 NPU를 포함한 다중 이종 처리 장치(PU)를 지원하는 딥러닝 컴파일러용 연산자 스케줄러인 PartitionTuner를 제안합니다. PartitionTuner는 전체 DNN 추론 시간을 최소화하기 위해 사용 가능한 모든 PU를 동시에 활용하는 연산자 스케줄링 계획을 생성할 수 있습니다. 연산자 스케줄링은 DNN 아키텍처 분석과 이종 처리 장치에서 측정된 개별 및 그룹 연산자의 성능 프로파일을 기반으로 합니다. 7개 DNN에 대한 실험 결과, PartitionTuner는 SqueezeNet에 대해 정적 타입 기반 연산자 스케줄링 기법보다 5.03% 더 우수한 성능을 보이는 스케줄링 계획을 생성합니다. 또한 PartitionTuner는 ResNet50, ResNet18, SqueezeNet에 대해 각각 7.18%, 5.36%, 2.73%로 최근 프로파일링 기반 연산자 스케줄링 기법보다 우수한 성능을 보입니다.}
}

% within 3 years

@inproceedings{kim2022cprune,
    abbr={ECCV},
    title={CPrune: Compiler-informed model pruning for efficient target-aware DNN execution},
    author={Kim, T. and Kwon, Yongin and Lee, Jemin and Kim, Taeho and Ha, Sangtae},
    booktitle={European Conference on Computer Vision (ECCV), pp.651–667, Oct. 23-27, 2022, NRF BK21+ IF: 2, Acceptance Rate <span class="label label-warning">28%</span> (1,650 papers accepted out of 5,803 submitted),
  doi: https://doi.org/10.1007/978-3-031-20044-1_37},
    pages={651--667},
    month = {10},
    year={2022},
    organization={Springer},
    selected={true},
    pdf = {https://doi.org/10.1007/978-3-031-20044-1_37},
    code = {https://github.com/taehokim20/CPrune},
    abstract = {모바일 기기에서는 이미지 분류, 음성 인식 등 다양한 목적을 위해 딥러닝 모델이 실행된다. 그러나 모바일 기기의 자원 제약으로 인해 연구자들은 모델 프루닝을 활용하여 경량화된 DNN 모델을 만들거나, 컴파일러 최적화를 통해 효율적인 코드를 생성하는 것에 주로 집중해 왔다.
놀랍게도, 우리는 모델 압축과 컴파일러 자동 튜닝(auto-tuning)을 단순히 결합하는 방식이 특정 타깃 장치에서 가장 효율적인 모델을 생성하지 못하는 경우가 많다는 사실을 발견했다.
이 문제를 해결하기 위해, 우리는 CPrune을 제안한다. CPrune은 목표 정확도를 충족해야 하는 애플리케이션을 지원하기 위해 타깃 장치에 최적화된 DNN 실행을 가능하게 하는 컴파일러 기반 정보 활용 프루닝(compiler-informed model pruning) 기법이다.
CPrune은 컴파일러 튜닝 과정에서 생성되는 서브그래프의 구조적 정보를 기반으로 프루닝을 수행하여 경량화된 DNN 모델을 만든다.
실험 결과, CPrune은 정확도 요구 조건을 충족하면서도 최첨단 TVM auto-tune 대비 최대 2.73배 빠른 DNN 실행 속도를 달성하였다.},
}

@article{park2022software,
    abbr={Access},
    title={Software-Level Memory Regulation to Reduce Execution Time Variation on Multicore Real-Time Systems},
    author={Park, Sihyeong and Lee, Jemin and Kim, Hyungshin},
    journal={IEEE Access Vol. 10 (JCR21 IF: <span class="label label-danger">3.476</span>), 1 Oct. 2022, ISSN: 2169-3536, doi: https://doi.org/10.1109/ACCESS.2022.3203702},
    volume={10},
    pages={93799--93811},
    year={2022},
    publisher={IEEE},
    pdf = {https://doi.org/10.1109/ACCESS.2022.3203702},
    abstract = {현대의 실시간 임베디드 시스템은 계산 집약적인 작업을 실행하기 위해 멀티코어 프로세서를 탑재하고 있다. 멀티코어 아키텍처에서는 마지막 단계의 캐시(last-level cache)가 여러 코어 간에 공유된다. 공유 캐시는 비결정적(non-deterministic) 자원이 되며, 이는 실시간 작업의 독립적인 실행에 영향을 미친다. 본 연구에서는 공유 캐시에서 간섭이 발생할 때 실행 시간의 변동을 완화하기 위한 해결책을 제안한다.
기존 방법들은 결정적인 실행 시간을 보장하기 위해 동시 메모리 접근을 회피하는 메모리 스케줄링 방식에 의존해 왔다. 그러나 이러한 방식은 최악 실행 시간(WCET)을 정확히 추정하기 위한 복잡한 분석이 필요하며, 결과적으로 지나치게 보수적인 방식으로 작업을 스케줄링해야 한다는 한계가 있다.
제안하는 방법은 기존 연구와 달리 복잡한 분석을 수행하지 않고, **메모리 배리어(memory barrier)의 부수 효과(side effect)**를 활용하여 동시 메모리 접근을 방지한다. 이를 위해 LLVM 컴파일러를 사용하여 기본 블록(basic block) 단위의 단순 코드 분석을 기반으로 메모리 배리어를 삽입한다.
제안된 방법은 운영체제나 작업 실행 흐름(task execution flow)을 수정할 필요가 없고, 분석 시간도 비교적 짧다. 방법의 유효성을 검증하기 위해, 멀티코어 환경에서 공유 캐시 간섭이 발생하는 상황을 가정하여 각 코어의 실행 시간 표준편차를 비교했다. 실험 결과, 제안하는 기본 블록 기반 메모리 배리어 삽입 방법은 간섭이 발생할 때 실행 시간의 변동을 최대 80%까지 감소시킬 수 있음을 확인하였다.}
}

@article{lee2022quantune,
  abbr={FGCS},
  title={Quantune: Post-training quantization of convolutional neural networks using extreme gradient boosting for fast deployment},
  author={Jemin Lee* and Yu, Misun and Kwon, Yongin and Kim, Taeho},
  journal={Future Generation Computer Systems Vol. 132, 2022, pp. 124-135, July 01, 2022 (JCR21 IF:
          <span class="label label-danger">7.307</span>, <span class="label label-warning">Top 9.09%</span>,
          computer science, theory & method rank <span class="label label-warning">#10</span> out of 110), ISBN: 0167-739X,
          doi: https://doi.org/10.1016/j.future.2022.02.005},
  volume={132},
  pages={124--135},
  day = {1},
  month = {7},
  year={2022},
  publisher={Elsevier},
  pdf = {https://www.sciencedirect.com/science/article/pii/S0167739X22000498?via%3Dihub},
  code = {https://github.com/etri/nest-compiler},
  code = {https://github.com/leejaymin/qaunt_xgboost},
  abstract = {다양한 자원 제약 환경에서 합성곱 신경망(CNN)을 활용하기 위해서는, 정밀도 표현을 더 낮은 비트 표현으로 변환하는 양자화(quantization) 과정을 통해 CNN 모델을 압축하는 것이 필수적이다. 그러나 양자화 과정에서 발생하는 데이터셋 민감도 문제, 높은 계산 요구량, 많은 시간 소모 등을 해결하기 위해, 재학습을 필요로 하지 않는 사후 양자화(post-training quantization) 기법들이 제안되어 왔다.
또한, 재학습 없이 발생하는 정확도 저하를 보완하기 위해, 기존 사후 양자화 연구에서는 보정(calibration), 스킴(schemes), 클리핑(clipping), 세분화(granularity), 혼합 정밀도(mixed-precision) 등 다양한 보완 방법을 제안해 왔다.
최소한의 오류로 양자화된 모델을 생성하기 위해서는, 이러한 방법들이 상호 보완적이며 CNN 모델마다 특성이 다르기 때문에, 가능한 모든 조합을 탐색할 필요가 있다. 하지만 이를 전부 탐색하는 방식(exhaustive search)은 너무 많은 시간이 들고, 휴리스틱 기반 탐색은 최적 해(suboptimal)에 머물 가능성이 크다.
이러한 문제를 해결하기 위해, 우리는 Quantune이라는 자동 튜너(auto-tuner)를 제안한다. Quantune은 **그래디언트 트리 부스팅 모델(gradient tree boosting model)**을 구축하여 양자화 구성(configurations) 탐색을 가속화하고, 양자화 오류를 줄인다.
Quantune은 랜덤 탐색, 그리드 탐색, 유전 알고리즘과 비교하여 평가되었다. 실험 결과, Quantune은 양자화 탐색 시간을 대폭 단축시키면서도, MobileNet, SqueezeNet, ShuffleNet과 같은 민감한 모델을 포함한 6개의 CNN 모델에서 0.07–0.65% 수준의 정확도 손실만을 보였다.
또한 Quantune은 다양한 타깃 장치를 지원하고 지속적으로 발전하는 양자화 연구를 수용하기 위해, 딥러닝용 통합 컴파일러 상에서 오픈소스로 구현되었다.}
}

@article{lee2022time,
  abbr={MDPI},
  title={Time-Invariant Features-Based Online Learning for Long-Term Notification Management: A Longitudinal Study},
  author={Lee, Jemin and Park, Sihyeong and Kim, Taeho and Kim, Hyungshin},
  journal={Applied Sciences Vol. 12, No. 11 Article-Num. 5432, June 01, 2022 (JCR21 IF: <span class="label label-danger">2.838</span>) ISSN: 2076-3417, doi: https://doi.org/10.3390/app12115432},
  volume={12},
  number={11},
  pages={5432},
  month = {6},
  year={2022},
  publisher={MDPI},
  abstract = {스마트폰과 웨어러블 기기에서 생성되는 일일 알림 수가 증가함에 따라 정신적 부담이 늘고, 생산성이 저하되며, 에너지가 낭비되는 문제가 나타나고 있다. 이러한 현상은 스마트폰, 스마트워치, 에어팟, 태블릿 등 사용자가 착용하거나 사용하는 개인 모바일 기기의 수가 증가함에 따라 더욱 심각해지고 있다. 여러 기기가 동시에 중복 알림을 생성할 수 있기 때문이다. 따라서 단순한 주의 분산뿐 아니라, 여러 기기가 유발하는 중복 알림은 에너지 낭비로 이어진다.
기존 연구에서는 PASS라는 알림 관리 시스템을 제안했으며, 이는 개인화된 모델을 기반으로 알림 발생을 자동으로 조절한다. 그러나 기존 연구에서는 시간 경과에 따른 사용자 행동 변화를 고려하지 않아, 머신러닝 기반 모델이 새로운 알림에 대해 제대로 작동하지 않는 문제가 있다.
모델이 장기간 사용될 때 발생하는 모델링과 실제 배치(deployment) 간의 성능 차이를 줄이기 위해, 우리는 장기간 데이터를 수집하는 **종단 연구(longitudinal study)**를 수행했다. 추가로 11,258개의 알림 데이터를 수집하여, 기존 데이터를 포함해 총 18,407개의 알림을 분석했으며, 전체 연구 기간은 2년에 걸친다.
통계적 검증을 통해 시간에 영향을 받지 않는(time-invariant) 특징을 식별했으며, 이를 모델 학습에 완전히 활용할 수 있음을 확인했다. 새롭게 발생하는 데이터로 인해 발생하는 정확도 하락 문제를 해결하기 위해, 우리는 시간 불변 윈도잉 온라인 학습(Windowing Time-Invariant Online Learning, WTOL) 기법을 설계했다. 새로 수집된 데이터셋에서 WTOL은 온라인 학습과 시간 민감도에 따른 윈도잉 특징을 결합하여, 기존 배치 학습 기반 모델의 F1-score를 44.3%에서 69.0%로 향상시켰다.}
}

% within 4 years


@article{lee2019pass,
    abbr={IEEE TMC},
    title={PASS: Reducing redundant notifications between a smartphone and a smartwatch for energy saving},
    author={Lee, Jemin and Lee, Uichin and Kim, Hyungshin},
    journal={IEEE Transactions on Mobile Computing,(impact factor: 5.538, JCR20: Top 17%, telecommunications rank #16 out of 91), ISSN: 1536-1233, doi: https://doi.org/10.1109/TMC.2019.2930506},
    volume={19},
    number={11},
    pages={2656--2669},
    month = {11},
    year={2020},
    publisher={IEEE},
    selected={true},
    pdf={https://doi.org/10.1109/TMC.2019.2930506},
    code = {https://github.com/leejaymin/nCollector}
}

@article{park2019hardware,
  abbr={MDPI},
  title={Hardware resource analysis in distributed training with edge devices},
  author={Park, Sihyeong and Lee, Jemin and Kim, Hyungshin},
  journal={Electronics Vol.9, Issue 1, pp.1-13, January 1, 2020 (JCR20 IF: <span class="label label-danger">2.397</span>), ISSN: 2079-9292,
        doi: https://doi.org/10.3390/electronics9010028},
  volume={9},
  number={1},
  pages={28},
  year={2019},
  publisher={MDPI},
  pdf = {https://www.mdpi.com/2079-9292/9/1/28}
}

@inproceedings{kang2019fire2,
    abbr={MobiCom},
    title={Fire in your hands: Understanding thermal behavior of smartphones},
    author={Kang, Soowon and Choi, Hyeonwoo and Park, Sooyoung and Park, Chunjong and Lee, Jemin and Lee, Uichin and Lee, Sung-Ju},
    booktitle={The 25th Annual International Conference on Mobile Computing and Networking, pp. 1-16, Los Cabos, Mexico, 21-25 Oct. 2019, NRF BK21+ IF: 4, Acceptance Rate <span class="label label-warning">19%</span> (55 papers accepted out of 290 submitted).},
    pages={1--16},
    month = {10},
    year={2019},
    selected={true},
    pdf = {https://dl.acm.org/citation.cfm?id=3300128},
    website = {https://nmsl.kaist.ac.kr/projects/thermal/},
    video = {https://youtu.be/mwiBnx4nr1w},
}

@inproceedings{kang2018understanding,
  abbr={UbiComp},
  title={Understanding Customers' Interests in the Wild},
  author={Kang, Soowon and Kim, Auk and Lee, Jemin and Shin, Ikhee and Lee, Uichin},
  booktitle={Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers (Poster)},
  pages={90--93},
  year={2018},
  pdf = {https://dl.acm.org/citation.cfm?id=3267625},
  poster = {SuggestBot_Poster_final.pdf},
}

@article{lee2018reducing,
  abbr={Hindawi},
  title={Reducing smartwatch users’ distraction with convolutional neural network},
  author={Lee, Jemin and Kwon, Jinse and Kim, Hyungshin and others},
  journal={Mobile Information Systems, vol. 2018, Article ID 7689549, 9 pages, 15 Mar. 2018 (special issue in Advances in Personalized Mobile Services). (impact factor(JCR18): <span class="label label-danger">1.635</span>), ISSN: 1574-017X (print), 1875-905X(online)},
  volume={2018},
  year={2018},
  publisher={Hindawi},
  pdf = {https://www.hindawi.com/journals/misy/aip/7689549/},
}

@article{lee2016qdroid,
  abbr={Hindawi},
  title={QDroid: Mobile application quality analyzer for app market curators},
  author={Lee, Jemin and Kim, Hyungshin},
  journal={Mobile Information Systems, vol. 2016, Article ID 1740129, 11 pages, 10 Oct. 2016. (impact factor(JCR15): <span class="label label-danger">1.462</span>), ISSN: 1574-017X (print), 1875-905X(online)},
  volume={2016},
  year={2016},
  publisher={Hindawi},
  pdf = {http://dx.doi.org/10.1155/2016/1740129},
  code = {https://github.com/leejaymin/QDroid},
}

@article{joe2017output,
  abbr={FGCS},
  title={Output-oriented power saving mode for mobile devices},
  author={Joe, Hyunwoo and Kim, Jungseok and Lee, Jemin and Kim, Hyungshin},
  journal={Future Generation Computer Systems, 6 Jun. 2016, ISSN 0167-739X.<br>(impact factor: <span class="label label-danger">2.430</span>, JCR-2015: <span class="label label-warning">Top 10%</span>, theory & methods category rank <span class="label label-warning">#11</span> out of 150)},
  volume={72},
  pages={49--64},
  year={2017},
  publisher={Elsevier},
  pdf = {http://dx.doi.org/10.1016/j.future.2016.05.012},
  video = {https://youtu.be/SOgCiU3rC34},
}

@article{lee2014automated,
  abbr={IEEE TCE},
  title={Automated power model generation method for smartphones},
  author={Lee, Jemin and Joe, Hyunwoo and Kim, Hyungshin},
  journal={IEEE Transactions on Consumer Electronics},
  volume={60},
  number={2},
  pages={190--197},
  year={2014},
  publisher={IEEE},
  pdf = {http://dx.doi.org/10.1109/TCE.2014.6851993},
  code = {https://github.com/PowerLab/PowerDoctor},
}

@inproceedings{park2017emsoftco,
  abbr={HENND},
  title={Analysis of Hardware Resources in Distributed Learning (Poster)},
  author={Sihyeong Park and Jemin Lee and Hyungshin Kim},
  booktitle={In Proceedings of International Workshop on Highly Efficient Neural Networks Design (co-located with EMSOFT), pp. 1-4, Seoul, South Korea, 20 Oct. 2017.},
  year={2017},
  organization={IEEE}
}

@inproceedings{Jinse17s,
  abbr={IEEE IPIN},
  title={An Ultrasound-based Indoor Localiztion Using Gaussian ASK Modulation (WIP)},
  author={Jinse Kwon and Jemin Lee and Hyungshin Kim},
  booktitle={In Proceedings of International Conference on Indoor Positioning and Indoor Navigation, pp.1-4, Sapporo, Japan, 20 Sept. 2017.},
  year={2017},
  organization={IEEE},
  pdf = {http://www.ipin2017.org/ipinpapers/225/225.pdf}
}


@inproceedings{park2017a,
  abbr={IEMEK},
  title={Deep Learning Training on Distributed Embedded Systems (Poster)},
  author={Sihyeong Park and Jemin Lee and Hyungshin Kim},
  booktitle={In Proceedings of the 12th lEMEK Symposium on Embedded Technology, Busan, South Korea, 18-19 May, 2017.},
  year={2017},
  organization={IEMEK},
  pdf = {iset2017.pdf},
  slides = {2017_ISET_park.pdf},
}

@inproceedings{Jinyoung2017a,
  abbr={IoTDI},
  title={Extending App Pre-Launch Service with Emotion Context (Poster)},
  author={Jinyoung Choi and Jemin Lee and Hyungshin Kim},
  booktitle={In Proceedings of the 2nd ACM/IEEE International Conference on Internet-of-Things Design and Implementation (IoTDI’17) Adjunct, pp. 1-2, Pittsburgh, USA, 18-21 Apr. 2017.},
  year={2017},
  organization={ACM/IEEE},
  pdf = {http://dl.acm.org/citation.cfm?id=3057306},
  slides = {2017_IoTDI_choi_Poster.pdf},
}


@inproceedings{lee2016a,
  abbr={MobileHCI},
  title={Reducing Distraction of Smartwatch Users with Deep Learning},
  author={Jemin Lee and Jinse Kwon and Hyungshin Kim},
  booktitle={In Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI'16) Adjunct pp. 948-953, Florence, Italy, 05-09 Sept. 2016.},
  year={2016},
  organization={ACM},
  pdf = {http://dl.acm.org/citation.cfm?id=2962662},
  slides = {Smarttention_2016_Slides.pdf},
}

@inproceedings{lee2013a,
  abbr={MobiSys},
  title={Framework for automated power estimation of Android applications (Poster)},
  author={Jemin Lee and Hyungshin Kim},
  booktitle={In Proceedings of the 11th annual international conference on Mobile Systems, Applications, and Services (<span class="label label-info">Mobisys'13</span>) Adjunct pp. 541-542, Taipei, Taiwan, Jun. 2013.},
  year={2013},
  organization={ACM},
  pdf = {http://dx.doi.org/10.1145/2462456.2465723},
  video = {http://youtu.be/jVffMVXG0KI},
}

@inproceedings{Vincent2012a,
  abbr={APSys},
  title={Energy Reservation Service for Smart Phone Application (Poster)},
  author={Vincent Dupre and Jemin Lee and Hyungshin Kim},
  booktitle={In Proceedings of 3rd ACM/SIGOPS Asia-Pacific Workshop on Systems (APSys'12), Seoul, South Korea, Jul. 2012.},
  year={2012},
  organization={ACM}
}

@inproceedings{lee2012a,
  abbr={ICCE},
  title={Smart Phone Power Model Generation Using Use Pattern Analysis},
  author={Jemin Lee and  Hyunwoo Joe and Hyungshin Kim},
  booktitle={In Proceedings of IEEE International Conference on Consumer Electronics (ICCE'12) pp. 412-413, Las Vegas, NV, USA, Jan. 2012.},
  year={2012},
  organization={IEEE},
  pdf = {http://dx.doi.org/10.1109/ICCE.2012.6161925},
  slides = {ICCE2012.pdf},
}


@inproceedings{jemin2011,
  abbr={EKC},
  title={Smartphone, where does the power go?},
  author={Jemin Lee and Hyunwoo Joe and Hyungshin Kim},
  booktitle={In Proceedings of EU Korea Conference on Science and Technology (EKC'11), Paris, France, Jul. 2011.},
  year={2011},
  organization={IEEE},
  pdf = {EKC2011_smartphone_fullpaper.pdf},
}

